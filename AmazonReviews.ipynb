{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6901a9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "import os\n",
    "import scipy.stats as st\n",
    "import random\n",
    "import string\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download([\"stopwords\",\"punkt\",\"names\"])\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70552986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the cookies and headers\n",
    "cookie = {'session-id':'261-0624820-8189244',\n",
    "        'ubid-acbuk':'257-5015255-6933932',\n",
    "        'x-acbuk':\"SHANeonOJaheMLxqVZ4msU4f4R@EmL3x\",\n",
    "        'at-acbuk':'Atza|IwEBIEnpF7WXlwtQsRyGZ7j2bTSRbzTBwDlAnOTyxuvo7Hvrhrn2dyp63ewuYjG4ia_InUZLsBocBwWU_fMQcC175R50rYypI0cd1zwygQibh3YcTNoC4w4rzORUj9CjWEky33VlVWjR03SMBLThsL1ThlCNlAbG2SSWBqYbpuYenZPGnGzsyRyykCwK-lWg_WvRnh8-ZbWAbCMCZnj4tQLVOdZD',\n",
    "        'sess-at-acbuk':'\"aCh5jaU0vQL4yngxb40ZUs3evR9B3Thcl34Fyde8oG8=\"',\n",
    "        'sst-acbuk':'Sst1|PQFNcV_AOW-PykmKjih9sdAzCYa21f7Wv9FmpIdzxnMwOp3RMKf9c5iGeatKfYULnwIwkDfn_otu-l7g3xLyxdFLuJWBpuIo23QnEpM21f00qFJD1SB3ocTbbSgowpR3HTSLghqfkiGttlxXKUjjI3jgntN-q2sJuWdSkHYdQTYLL_4f2UAf6oENeuPJ9eXQfhi1TTmSBJx4GEvELHlMhuNCz7hGEB9k_JU8fP1qs6bAQnQIQ5v8m7UTGFawlDa7JwoSbr-aojXr8KbJA8K981Kxpk8mSy3tsDg77AiBzOsMY6M',\n",
    "        'i18n-prefs':'GBP',\n",
    "        'lc-acbuk':'en_GB',\n",
    "        'av-timezone':'Europe/London',\n",
    "        'session-token':'\"+VfkDfO9fgKFPdKHAQnlKJz/jM7u+awmBduH5/PFl5ztXolz282+s/gJig/ma6wa199MlP38l+UB34qpiitev5XFOb5Tn7dRH5cnTCHuVrWu5NwEOQY9Z9PjAgXHSrTMNYRsNhf/5ehxk6RvDP/k62m3wvdxLv3FMQUkEy2kkqxkUhpJHuloo7YsSgiApoNh2M3qQykGoL1pnMX29K252A==\"',\n",
    "        'session-id-time':'2082758401l'\n",
    "        }\n",
    "\n",
    "header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da97671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all URLs of pages\n",
    "def getURLs(page1):\n",
    "    urls = []\n",
    "    page_stem = page1[:-1]\n",
    "    starting_url = page1\n",
    "    page = requests.get(starting_url,headers = header)\n",
    "    print(page)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    num_reviews = soup.findAll(\"div\",{'data-hook':'cr-filter-info-review-rating-count'})\n",
    "    num_pages = 0\n",
    "    for result in num_reviews:\n",
    "        result_string = str(result)\n",
    "        result_string_clean = re.sub('<[^>]+>', '', result_string)\n",
    "        result_stripped = result_string_clean.strip()\n",
    "        result_substring = result_stripped.split(\"| \")[1].split(\" \")[0]\n",
    "        num_pages = math.ceil(int(result_substring)/10)\n",
    "\n",
    "    for i in range(num_pages):\n",
    "        url = page_stem + str(i)\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93864196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReviewsFromURL(url):\n",
    "    page = requests.get(str(url),cookies=cookie,headers = header)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    reviews = soup.findAll(\"span\", {'data-hook':\"review-body\"})\n",
    "    dates = soup.findAll(\"span\", {'data-hook':\"review-date\"})\n",
    "    return reviews,dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45cb972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllReviews(urls):\n",
    "    all_reviews = []\n",
    "    for url in urls:\n",
    "        reviews,dates = getReviewsFromURL(url)\n",
    "        for review,date in zip(reviews,dates):\n",
    "            review_string = str(review)\n",
    "            date_string = str(date).split(\"on \",1)[1][:-7]\n",
    "            review_string_clean = re.sub('<[^>]+>', '', review_string)\n",
    "            review_string_cleaner = re.sub(r'http\\S+', '', review_string_clean)\n",
    "            date_string_clean = re.sub('<[^>]+>', '', date_string)\n",
    "            review_stripped = review_string_cleaner.strip()\n",
    "            date_stripped = date_string_clean.strip()\n",
    "            date_formatted = datetime.strptime(date_stripped, \"%d %B %Y\")\n",
    "            all_reviews.append((review_stripped,date_formatted))\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cceda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeReviews(product_reviews):\n",
    "    tokenized_reviews = []\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    for review,date in tqdm(product_reviews):\n",
    "        review = review.replace(\"n't\",\"not\")\n",
    "        tokenized_review  = [word.lower() for word in word_tokenize(review) if word.lower() not in stopwords and word.lower() not in string.punctuation and word.lower() not in [\"’\",\"``\",\"...\",\"....\",\"''\",\"amp\"]]\n",
    "        if len(tokenized_review) != 0:\n",
    "            tokenized_reviews.append((tokenized_review,date))\n",
    "    return tokenized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17ec34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeWords(tokenized_reviews):\n",
    "    tokenized_words = []\n",
    "\n",
    "    for review,date in tokenized_reviews:\n",
    "        for word in review:\n",
    "            tokenized_words.append(word)\n",
    "    \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce259a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTargetDirectory(product):\n",
    "    newpath = \"Figures/\" + product\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22e89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall frequency analysis\n",
    "def singleWordCount(tokenized_reviews):\n",
    "    tokenized_words = tokenizeWords(tokenized_reviews)\n",
    "    \n",
    "    fd = nltk.FreqDist(tokenized_words)\n",
    "\n",
    "    single_words = []\n",
    "    single_counts = []\n",
    "\n",
    "    for word,count in fd.most_common(25):\n",
    "        single_words.append(word)\n",
    "        single_counts.append(count)\n",
    "    single_words.append(\"different\")\n",
    "    single_counts.append([fd[\"different\"]])\n",
    "    return single_words,single_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75202935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams\n",
    "def pairWordCount(tokenized_reviews):\n",
    "    tokenized_words = tokenizeWords(tokenized_reviews)\n",
    "\n",
    "    bi_finder = nltk.collocations.BigramCollocationFinder.from_words(tokenized_words)\n",
    "    \n",
    "    double_words = []\n",
    "    double_counts = []\n",
    "    \n",
    "    for word,count in bi_finder.ngram_fd.most_common(5):\n",
    "        double_words.append(word[0] + \", \" + word[1])\n",
    "        double_counts.append(count)\n",
    "    return double_words,double_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e0a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigrams\n",
    "def tripletWordCount(tokenized_reviews):\n",
    "    tokenized_words = tokenizeWords(tokenized_reviews)\n",
    "\n",
    "    tri_finder = nltk.collocations.TrigramCollocationFinder.from_words(tokenized_words)\n",
    "    \n",
    "    triple_words = []\n",
    "    triple_counts = []\n",
    "\n",
    "    for word,count in tri_finder.ngram_fd.most_common(5):\n",
    "        triple_words.append(word[0] + \", \" + word[1] + \", \" + word[2])\n",
    "        triple_counts.append(count)\n",
    "    return triple_words,triple_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46da1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency graphs\n",
    "def generateFrequencyGraph(tokenized_reviews,product):\n",
    "    \n",
    "    single_words,single_counts = singleWordCount(tokenized_reviews)\n",
    "    pair_words,pair_counts = pairWordCount(tokenized_reviews)\n",
    "    triplet_words,triplet_counts = tripletWordCount(tokenized_reviews)\n",
    "    \n",
    "    colors1 = []\n",
    "    colors2 = ['#00876c','#4c9c85','#78b19f','#a0c6b9','#c8dbd5']\n",
    "    colors3 = ['#d43d51','#df676e','#e88b8d','#eeadad','#f1cfce']\n",
    "\n",
    "    for i in range(25):\n",
    "        dark = (0, 76, 109)\n",
    "        dark_r = dark[0]\n",
    "        dark_g = dark[1]\n",
    "        dark_b = dark[2]\n",
    "        light = (192, 228, 255)\n",
    "        light_r = light[0]\n",
    "        light_g = light[1]\n",
    "        light_b = light[2]\n",
    "\n",
    "        diff_r = light_r - dark_r\n",
    "        diff_g = light_g - dark_g\n",
    "        diff_b = light_b - dark_b\n",
    "\n",
    "        modifier = i/25\n",
    "        new_color = ((dark_r + (diff_r*modifier))/255,(dark_g + (diff_g*modifier))/255,(dark_b + (diff_b*modifier))/255)\n",
    "        colors1.append(new_color)\n",
    "        \n",
    "    colors1.append((1,0.9,0.7))\n",
    "\n",
    "    fig1 = plt.figure(figsize=(20,12))\n",
    "    ax1 = fig1.add_subplot(2,1,1)\n",
    "    plt.bar(range(len(single_counts)),single_counts,tick_label=single_words,color = colors1)\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.title(f\"Single Word Frequency in {product} Amazon Reviews - {len(tokenized_reviews)} reviews\")\n",
    "    ax1.set_ylim([0,math.floor(single_counts[0] * 1.1)])\n",
    "    ax2 = fig1.add_subplot(2,2,3)\n",
    "    plt.bar(range(len(pair_counts)),pair_counts,tick_label=pair_words,color = colors2)\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.title(f\"Paired Word Frequency in {product} Amazon Reviews - {len(tokenized_reviews)} reviews\")\n",
    "    ax2.set_ylim([0,math.floor(single_counts[0] * 1.1)])\n",
    "    ax3 = fig1.add_subplot(2,2,4)\n",
    "    plt.bar(range(len(triplet_counts)),triplet_counts,tick_label=triplet_words,color = colors3)\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.title(f\"Triplet Word Frequency in {product} Amazon Reviews - {len(tokenized_reviews)} reviews\")\n",
    "    ax3.set_ylim([0,math.floor(single_counts[0] * 1.1)])\n",
    "    fig1.savefig(f\"Figures/{product}/{product}_frequency\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c692ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentimentAnalysis(tokenized_reviews,product):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiments = []\n",
    "    grouped_sentiments = {\n",
    "        \"strong negative\":0,\n",
    "        \"negative\":0,\n",
    "        \"slight negative\":0,\n",
    "        \"neutral\":0,\n",
    "        \"slight positive\":0,\n",
    "        \"positive\":0,\n",
    "        \"strong positive\":0\n",
    "    }\n",
    "\n",
    "    for review,date in tokenized_reviews:\n",
    "        sentiments.append(sia.polarity_scores(\" \".join(review))[\"compound\"])\n",
    "\n",
    "    for sentiment in sentiments:\n",
    "        if sentiment < -0.65:\n",
    "            grouped_sentiments[\"strong negative\"] += 1\n",
    "        elif sentiment < -0.35:\n",
    "            grouped_sentiments[\"negative\"] += 1\n",
    "        elif sentiment < -0.1:\n",
    "            grouped_sentiments[\"slight negative\"] += 1\n",
    "        elif sentiment < 0.1:\n",
    "            grouped_sentiments[\"neutral\"] += 1\n",
    "        elif sentiment < 0.35:\n",
    "            grouped_sentiments[\"slight positive\"] += 1\n",
    "        elif sentiment < 0.65:\n",
    "            grouped_sentiments[\"positive\"] += 1\n",
    "        else:\n",
    "            grouped_sentiments[\"strong positive\"] += 1\n",
    "\n",
    "    sentiment_labels = []\n",
    "    category_definitions = [\"-1 to -0.66\",\"-0.65 to -0.36\",\"-0.35 to -0.11\",\"-0.1 to 0.09\",\"0.1 to 0.34\",\"0.35 to 0.64\",\"0.65 to 1\"]\n",
    "    sentiment_counts = []\n",
    "\n",
    "    for category,count in grouped_sentiments.items():\n",
    "            sentiment_labels.append(category)\n",
    "            sentiment_counts.append(count)\n",
    "\n",
    "    diverging_colors = ['#00876c',\n",
    "    '#6aaa96',\n",
    "    '#aecdc2',\n",
    "    '#f1f1f1',\n",
    "    '#f0b8b8',\n",
    "    '#e67f83',\n",
    "    '#d43d51']\n",
    "\n",
    "    reversed_colors = []\n",
    "    for color in diverging_colors:\n",
    "        reversed_colors.insert(0,color)\n",
    "\n",
    "    fig2 = plt.figure(figsize = (10,10))\n",
    "    plt.bar(range(len(sentiment_counts)),sentiment_counts,tick_label = sentiment_labels,color = reversed_colors)\n",
    "\n",
    "    patches = []\n",
    "\n",
    "    for i in range(len(sentiment_labels)):\n",
    "        patches.append(mpatches.Patch(color=reversed_colors[i], label=category_definitions[i]))\n",
    "\n",
    "    plt.legend(handles=patches)\n",
    "    plt.title(f\"Sentiment Analysis of {product} Amazon reviews - {len(tokenized_reviews)} reviews\")\n",
    "    fig2.savefig(f\"Figures/{product}/{product}_sentiment\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e63986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentimentOverTime(tokenized_reviews,product):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    sentiments = []\n",
    "    for review,date in tokenized_reviews:\n",
    "        sentiments.append(sia.polarity_scores(\" \".join(review))[\"compound\"])\n",
    "    \n",
    "    today = datetime.now()\n",
    "    time_grouped_sentiments = {}\n",
    "\n",
    "    def diff_month(d1, d2):\n",
    "        return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "    for i in range(len(tokenized_reviews)):\n",
    "        age_months = diff_month(today,tokenized_reviews[i][1])\n",
    "        if age_months < 16:\n",
    "            if age_months in time_grouped_sentiments:\n",
    "                time_grouped_sentiments[age_months].append(sentiments[i])\n",
    "            else:\n",
    "                time_grouped_sentiments[age_months] = [sentiments[i]]\n",
    "\n",
    "    months_old = []\n",
    "    avg_sentiments = []\n",
    "    errors_high = []\n",
    "    errors_low = []\n",
    "\n",
    "    for key in time_grouped_sentiments:\n",
    "        values = time_grouped_sentiments[key]\n",
    "        months_old.append(int(key))\n",
    "        mean = statistics.mean(values)\n",
    "        avg_sentiments.append(mean)\n",
    "        standard_error = st.sem(values)\n",
    "        degrees_freedom = len(values)-1\n",
    "        confidence_interval = st.t.interval(0.95, degrees_freedom, mean, standard_error)\n",
    "        if math.isnan(confidence_interval[0]) or math.isnan(confidence_interval[1]):\n",
    "            confidence_interval = (mean,mean)\n",
    "        errors_low.append(confidence_interval[0])\n",
    "        errors_high.append(confidence_interval[1])\n",
    "        #print(f\"Key: {key} Mean: {mean} Interval: {confidence_interval}\")\n",
    "\n",
    "    order = np.argsort(months_old)\n",
    "    months_old_sorted = np.array(months_old)[order]\n",
    "    avg_sentiments_sorted = np.array(avg_sentiments)[order]\n",
    "    errors_high_sorted = np.array(errors_high)[order]\n",
    "    errors_low_sorted = np.array(errors_low)[order]\n",
    "\n",
    "    fig4 = plt.figure(figsize=(10,10))\n",
    "    plt.plot(months_old_sorted,avg_sentiments_sorted,color='#d43d51')\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "    ax.set_ylim([-1,1])\n",
    "    plt.xlabel(\"Age of Review (Months)\")\n",
    "    plt.ylabel(\"Average Sentiment Score\")\n",
    "    plt.title(f\"Change in Sentiment of {product} Amazon Reviews Over Time\")\n",
    "    ax.fill_between(months_old_sorted,errors_low_sorted,errors_high_sorted,color='#d43d51',alpha=0.2)\n",
    "    fig4.savefig(f\"Figures/{product}/{product}_sentiment_over_time\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1b8663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud of the amazon reviews\n",
    "def generateWordCloud(tokenized_reviews,product):\n",
    "    colorList = [(205, 33, 56),\n",
    "                 (206, 48, 72),\n",
    "                 (357, 65, 64),\n",
    "                 (359, 67, 73)]\n",
    "    \n",
    "    def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "        seed = random.randint(0,len(colorList)-1)\n",
    "        color = colorList[seed]\n",
    "        h = color[0]\n",
    "        s = color[1]\n",
    "        l = color[2]\n",
    "        return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
    "    \n",
    "    tokenized_words = tokenizeWords(tokenized_reviews)\n",
    "    \n",
    "    fig3 = plt.figure(figsize=(16,8))\n",
    "    wc = wordcloud = WordCloud(width=2000, height=1000,font_path=r'C:\\Windows\\Fonts\\Calibri.ttf',max_words = 100,background_color = \"white\",color_func=random_color_func).generate(\" \".join(tokenized_words))\n",
    "    plt.imshow(wc,interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    fig3.savefig(f\"Figures/{product}/{product}_word_cloud\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bd4fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performAnalysis(product_url,product_name):\n",
    "    all_product_url = getURLs(product_url)\n",
    "    product_reviews = getAllReviews(all_product_url)\n",
    "    product_tokenized_reviews = tokenizeReviews(product_reviews)\n",
    "    createTargetDirectory(product_name)\n",
    "    generateFrequencyGraph(product_tokenized_reviews,product_name)\n",
    "    generateSentimentAnalysis(product_tokenized_reviews,product_name)\n",
    "    generateSentimentOverTime(product_tokenized_reviews,product_name)\n",
    "    generateWordCloud(product_tokenized_reviews,product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "004426d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 210/210 [00:00<00:00, 5037.45it/s]\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 220/220 [00:00<00:00, 6308.04it/s]\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 780/780 [00:00<00:00, 5284.34it/s]\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\Users\\Matth\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrunning = True\\nwhile running == True:\\n    input_name = input(\"Enter the product name:\")\\n    input_url = input(\"Paste the url of page 1 of the reviews here:\")\\n    performAnalysis(input_url,input_name)\\n    no_accepted_input = True\\n    while no_accepted_input:\\n        user_continue = input(\"Would you like to perform another analysis? (yes/no)\")\\n        if user_continue == \"no\":\\n            no_accepted_input = False\\n            running = False            \\n        elif user_continue == \"yes\":\\n            no_accepted_input = False\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flarin_url = \"https://www.amazon.co.uk/FLARIN-Joint-Muscular-Relief-200mg/product-reviews/B00FOJF76E/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1\"\n",
    "nurofen_url = \"https://www.amazon.co.uk/Nurofen-Tablets-200-mg-16/product-reviews/B001DXNRZ8/ref=cm_cr_getr_d_paging_btm_prev_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1\"\n",
    "voltarol_url = \"https://www.amazon.co.uk/Voltarol-Joint-Pain-2-32-Relief/product-reviews/B07HLR7PPZ/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=1\"\n",
    "\n",
    "performAnalysis(flarin_url,\"flarin\")\n",
    "performAnalysis(nurofen_url,\"nurofen\")\n",
    "performAnalysis(voltarol_url,\"voltarol\")\n",
    "\"\"\"\n",
    "running = True\n",
    "while running == True:\n",
    "    input_name = input(\"Enter the product name:\")\n",
    "    input_url = input(\"Paste the url of page 1 of the reviews here:\")\n",
    "    performAnalysis(input_url,input_name)\n",
    "    no_accepted_input = True\n",
    "    while no_accepted_input:\n",
    "        user_continue = input(\"Would you like to perform another analysis? (yes/no)\")\n",
    "        if user_continue == \"no\":\n",
    "            no_accepted_input = False\n",
    "            running = False            \n",
    "        elif user_continue == \"yes\":\n",
    "            no_accepted_input = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424caea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
